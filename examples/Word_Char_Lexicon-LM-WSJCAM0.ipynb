{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup notebook, imports and predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nhpylm.lexicon import build_fst_for_lexicon\n",
    "from nhpylm import fst\n",
    "import os\n",
    "from nhpylm.c_core import nhpylm\n",
    "from tqdm import tqdm\n",
    "from nhpylm.kaldi_data_preparation import convert_transcription, word_to_grapheme\n",
    "import json\n",
    "from nhpylm import json_utils as ju"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = 'lattice_playground/wcl-l-wsjcam0/'\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined write and print/display function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_and_print_fst(fst_graph, fst_filename, sym_filename, **kwargs):\n",
    "    print_fst_kwargs = {'determinize': False}\n",
    "    print_fst_kwargs.update(kwargs)\n",
    "    fst_graph.write_fst(fst_filename, **print_fst_kwargs)\n",
    "    return fst.print(fst_filename, sym_filename, sym_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a list of sentences to a list of list of list of units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_splitted_words(text):\n",
    "    return [convert_transcription(line, word_to_units=word_to_grapheme(join=False))[1] for line in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all unique units from the converted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_symbols(text):\n",
    "    return {symbol for line in text for word in line for symbol in word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a symbol list to a symbol file mapping all symbols to integers from 1 to N_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_symbols(symbols, sym_file):\n",
    "    with open(sym_file, 'w') as fid:\n",
    "        for i, s in enumerate(symbols):\n",
    "            fid.write('{} {}\\n'.format(s, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database file and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database_file = '/net/storage/database_jsons/reverb.json'\n",
    "train_flist = 'train/flists/wav/si_tr'\n",
    "test_flist = 'dev/flists/wav/si_dt5b'\n",
    "tlist = 'orth'\n",
    "channels = ['observed/CH1']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load database (filelist and transcription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(database_file) as fid:\n",
    "    database = json.load(fid)\n",
    "\n",
    "train_files = ju.traverse_to_dict(database, train_flist)\n",
    "train_files_for_channel = {channel: ju.get_flist_for_channel(train_files, channel) for channel in channels}\n",
    "test_files = ju.traverse_to_dict(database, test_flist)\n",
    "test_files_for_channel = {channel: ju.get_flist_for_channel(test_files, channel) for channel in channels}\n",
    "transcriptions = ju.traverse_to_dict(database, tlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get train data, split into characters and get symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = [transcriptions[key] for key in train_files_for_channel[channels[0]].keys()]\n",
    "train_data_splitted = text_to_splitted_words(train_data)\n",
    "symbols = find_symbols(train_data_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate LM, add training sentences and resample hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "lm = nhpylm.NHPYLM_wrapper(list(symbols), 2, 8)\n",
    "\n",
    "train_data_ids = lm.word_lists_to_id_lists(train_data_splitted)\n",
    "for line in tqdm(train_data_ids):\n",
    "    lm.add_id_sentence_to_lm(line)\n",
    "    \n",
    "lm.resample_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write symbol file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sym_filename = output_directory + 'symbols.txt'\n",
    "word_list = lm.string_ids\n",
    "write_symbols(word_list, sym_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get int versions of lexicon an labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_lexicon = lm.get_word_id_to_char_id()\n",
    "int_labels = lm.get_char_ids()\n",
    "int_eps = lm.sym2id('EPS')\n",
    "int_eow = lm.sym2id('EOW')\n",
    "int_eoc = lm.sym2id('EOC')\n",
    "int_phi = lm.sym2id('PHI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and write FST for lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "L_fst_filename = output_directory + 'L.fst'\n",
    "mode = 'trie'\n",
    "build_character_model = True\n",
    "fst_lexicon = build_fst_for_lexicon(int_lexicon, int_eps, int_eow, build_character_model,\n",
    "                                    mode, int_labels, eoc=int_eoc)\n",
    "fst_lexicon.add_self_loops(int_phi)\n",
    "fst_lexicon.write_fst(L_fst_filename, minimize=False, determinize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get FST for language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "G_fst_filename = output_directory + 'G.fst'\n",
    "_, arc_list = lm.to_fst_text_format(sow=int_eps, eow=int_eoc, eos_word=int_eps)\n",
    "G_fst = fst.build_fst_from_arc_list(arc_list)\n",
    "G_fst.write_fst(G_fst_filename, minimize=False, determinize=False, rmepsilon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose lexicon and language model FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_G_fst_filename = output_directory + 'L_G.fst'\n",
    "fst.compose(L_fst_filename, G_fst_filename, L_G_fst_filename,\n",
    "            determinize=False, minimize=False, rmepsilon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_data = [transcriptions[key] for key in test_files_for_channel[channels[0]].keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some character sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "character_sequence = ''.join(test_data[0].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and print FST for character sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"lattice_playground/wcl-l-wsjcam0/I.fst.pdf\" width=1000 height=400></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{lattice_playground/wcl-l-wsjcam0/I.fst.pdf}"
      ],
      "text/plain": [
       "<nhpylm.display_pdf.PDF at 0x7f8310e4bc88>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int_sequence = [lm.sym2id(character) for character in character_sequence]\n",
    "\n",
    "I_fst_filename = output_directory + 'I.fst'\n",
    "character_sequence_fst = fst.build_fst_for_sequence(int_sequence)\n",
    "write_and_print_fst(character_sequence_fst, I_fst_filename, sym_filename, minimize=False, determinize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add loops for word end/disambigutity symbols (eow and eoc) and fall back (phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"lattice_playground/wcl-l-wsjcam0/I_loop.fst.pdf\" width=1000 height=400></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{lattice_playground/wcl-l-wsjcam0/I_loop.fst.pdf}"
      ],
      "text/plain": [
       "<nhpylm.display_pdf.PDF at 0x7f8310e4bc50>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_loop_fst_filename = output_directory + 'I_loop.fst'\n",
    "character_sequence_fst.add_self_loops(int_eps, int_eow, int_eow, mode='after')\n",
    "character_sequence_fst.add_self_loops(int_eps, int_eoc, int_eoc, mode='after')\n",
    "character_sequence_fst.add_self_loops(int_eps, int_phi, int_phi, mode='before')\n",
    "write_and_print_fst(character_sequence_fst, I_loop_fst_filename, sym_filename, minimize=False, determinize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do the final compositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compose input sequence FST with lexicon and language model FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"lattice_playground/wcl-l-wsjcam0/I_loop_L_G.fst.pdf\" width=1000 height=400></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{lattice_playground/wcl-l-wsjcam0/I_loop_L_G.fst.pdf}"
      ],
      "text/plain": [
       "<nhpylm.display_pdf.PDF at 0x7f8310e4bf28>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_loop_L_G_fst_filename = output_directory + 'I_loop_L_G.fst'\n",
    "fst.compose(I_loop_fst_filename, L_G_fst_filename, I_loop_L_G_fst_filename,\n",
    "            determinize=False, minimize=False, rmepsilon=False)\n",
    "fst.print(I_loop_L_G_fst_filename, sym_filename, sym_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get shortest path(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe src=\"lattice_playground/wcl-l-wsjcam0/I_loop_L_G_shortestpath.fst.pdf\" width=1000 height=400></iframe>"
      ],
      "text/latex": [
       "\\includegraphics[width=1.0\\textwidth]{lattice_playground/wcl-l-wsjcam0/I_loop_L_G_shortestpath.fst.pdf}"
      ],
      "text/plain": [
       "<nhpylm.display_pdf.PDF at 0x7f8310e4b278>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I_loop_L_G_shortestpath_fst_filename = output_directory + 'I_loop_L_G_shortestpath.fst'\n",
    "fst.shortestpath(I_loop_L_G_fst_filename, I_loop_L_G_shortestpath_fst_filename, nshortest=1,\n",
    "            determinize=False, minimize=False, rmepsilon=True, project=True, project_output=True)\n",
    "fst.print(I_loop_L_G_shortestpath_fst_filename, sym_filename, sym_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
