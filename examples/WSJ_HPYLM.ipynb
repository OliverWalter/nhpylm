{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup notebook, imports and predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nhpylm.lexicon import build_fst_for_lexicon\n",
    "import os\n",
    "from nhpylm.c_core import nhpylm\n",
    "from tqdm import tqdm\n",
    "from nhpylm import fst\n",
    "from nhpylm.kaldi_data_preparation import convert_transcription, word_to_grapheme\n",
    "import json\n",
    "from nhpylm import json_utils as ju\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_directory = 'lattice_playground/wsj_hpylm/'\n",
    "os.makedirs(output_directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some predefined functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert a list of sentences to a list of list of list of units:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def text_to_splitted_words(text):\n",
    "    return [convert_transcription(line, word_to_units=word_to_grapheme(join=False))[1] for line in text]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all unique units from the converted sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_symbols(text):\n",
    "    return {symbol for line in text for word in line for symbol in word}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a symbol list to a symbol file mapping all symbols to integers from 1 to N_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def write_symbols(symbols, sym_file):\n",
    "    with open(sym_file, 'w') as fid:\n",
    "        for i, s in enumerate(symbols):\n",
    "            fid.write('{} {}\\n'.format(s, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return all files ending with '.z' in specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_z_files(path):\n",
    "    z_files = list()\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            if file.endswith('.z'):\n",
    "                z_files.append(os.path.join(root, file))\n",
    "    return z_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read a '.z' file and return uncompressed output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_z_file(filepath):\n",
    "    p = subprocess.Popen(['gunzip', '-c', filepath], stdout=subprocess.PIPE, universal_newlines=True)\n",
    "    text, _ = p.communicate()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text lines from WSJ training data and return only training sentence (upper case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wsj_lm_text_to_lm_lines(text):\n",
    "    lines = list()\n",
    "    for line in text.replace('</p>', '').split('</s>'):\n",
    "        lines.append(line.strip().split('\\n')[-1].upper())\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read all lm files ('.z' files) from specified path (lm training data in WSJ databse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_lm_files(path):\n",
    "    data = list()\n",
    "    for file in get_z_files(path):\n",
    "        data.extend(wsj_lm_text_to_lm_lines(read_z_file(file)))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Database file and paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "database_path = '/net/speechdb/wsj/11_13_1/wsj0/doc/lng_modl/lm_train/np_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get train data, split into characters and get symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data = read_lm_files(database_path)\n",
    "train_data_splitted = text_to_splitted_words(train_data)\n",
    "symbols = find_symbols(train_data_splitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate LM, add training sentences and resample hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "lm = nhpylm.NHPYLM_wrapper(list(symbols), 2, 8)\n",
    "\n",
    "train_data_ids = lm.word_lists_to_id_lists(train_data_splitted)\n",
    "for line in tqdm(train_data_ids):\n",
    "    lm.add_id_sentence_to_lm(line)\n",
    "    \n",
    "lm.resample_hyperparameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write symbol file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sym_filename = output_directory + 'symbols.txt'\n",
    "\n",
    "word_list = lm.string_ids\n",
    "write_symbols(word_list, sym_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get int lexicon and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "int_lexicon = lm.get_word_id_to_char_id()\n",
    "int_labels = lm.get_char_ids()\n",
    "int_eps = lm.sym2id('EPS')\n",
    "int_eow = lm.sym2id('EOW')\n",
    "int_eoc = lm.sym2id('EOC')\n",
    "int_phi = lm.sym2id('PHI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build and write FST for lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "L_fst_filename = output_directory + 'L.fst'\n",
    "mode = 'trie'\n",
    "build_character_model = True\n",
    "fst_lexicon = build_fst_for_lexicon(int_lexicon, int_eps, int_eow, build_character_model,\n",
    "                                    mode, int_labels, eoc=int_eoc)\n",
    "fst_lexicon.add_self_loops(int_phi)\n",
    "fst_lexicon.write_fst(L_fst_filename, minimize=True, determinize=False, rmepsilon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get FST for language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G_fst_filename = output_directory + 'G.fst'\n",
    "_, arc_list = lm.to_fst_text_format(sow=int_eps, eow=int_eoc, eos_word=int_eps)\n",
    "G_fst = fst.build_fst_from_arc_list(arc_list)\n",
    "G_fst.write_fst(G_fst_filename, minimize=True, rmepsilon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compose lexicon and language model FST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "L_G_fst_filename = output_directory + 'L_G.fst'\n",
    "fst.compose(L_fst_filename, G_fst_filename, L_G_fst_filename,\n",
    "            determinize=False, minimize=True, rmepsilon=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
